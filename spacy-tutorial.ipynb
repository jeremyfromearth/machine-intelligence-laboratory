{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaCy Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import IS_PUNCT, LOWER\n",
    "from spacy.symbols import nsubj, NOUN, PUNCT, DET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"spaCy features a fast and accurate syntactic \"\\\n",
    "    \"dependency parser, and has a rich API for navigating the tree. \" \\\n",
    "    \"The parser also powers the sentence boundary detection, \"\\\n",
    "    \"and lets you iterate over base noun phrases, or 'chunks'.\"\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS (Part of Speech Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NOUN', spaCy), ('VERB', features), ('ADJ', fast), ('CCONJ', and), ('ADJ', accurate), ('ADJ', syntactic), ('NOUN', dependency), ('NOUN', parser), ('CCONJ', and), ('VERB', has), ('ADJ', rich), ('NOUN', API), ('ADP', for), ('VERB', navigating), ('NOUN', tree), ('NOUN', parser), ('ADV', also), ('VERB', powers), ('NOUN', sentence), ('ADJ', boundary), ('NOUN', detection), ('CCONJ', and), ('VERB', lets), ('PRON', you), ('VERB', iterate), ('ADP', over), ('NOUN', base), ('NOUN', noun), ('NOUN', phrases), ('CCONJ', or), ('NOUN', chunks)]\n"
     ]
    }
   ],
   "source": [
    "pairs = []\n",
    "for token in doc:\n",
    "    # Only print toknes that are not punctuation or determiners\n",
    "    if token.pos is not PUNCT and token.pos is not DET:\n",
    "        pairs.append((token.pos_, token))\n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy features a fast and accurate syntactic dependency parser, and has a rich API for navigating the tree. \n",
      "The parser also powers the sentence boundary detection, and lets you iterate over base noun phrases, or 'chunks'.\n"
     ]
    }
   ],
   "source": [
    "for token in doc.sents:\n",
    "    print(token.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to Naive Regex Sentence Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cy features a fast and accurate syntactic dependency parser, and has a rich API for navigating the tree.\n",
      "The parser also powers the sentence boundary detection, and lets you iterate over base noun phrases, or 'chunks'.\n"
     ]
    }
   ],
   "source": [
    "sentence_pattern = re.compile(r'([A-Z].*?[\\.!?])', re.M)\n",
    "sentences = sentence_pattern.findall(s)\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG API\n"
     ]
    }
   ],
   "source": [
    "for token in doc.ents:\n",
    "    print(token.label_, token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Arctic Fox wiki article and process the extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "arctic_fox = json.loads(open('./wikipedia/arctic-fox.json').read().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME\n",
      "\t 12\n",
      "GPE\n",
      "\t canada\n",
      "\t lapland\n",
      "\t iceland\n",
      "\t russia\n",
      "\t sweden\n",
      "\t finland\n",
      "\t norway\n",
      "PERCENT\n",
      "\t more than\n",
      "\t more than 50%.\n",
      "DATE\n",
      "\t many decades\n",
      "\t 1996\n",
      "\t summer\n",
      "\t 90 years\n",
      "\t 2005\n",
      "\t the last decade\n",
      "\t the 10th edition\n",
      "\t the end of the last ice\n",
      "\t 25 to 30\n",
      "\t several decades\n",
      "\t between years due to the large population fluctuations\n",
      "\t winter\n",
      "\t april and may\n",
      "\t 9 weeks of age\n",
      "\t five to eight kits\n",
      "\t each day\n",
      "\t 46 to 68 cm\n",
      "\t 1997\n",
      "\t many generations\n",
      "\t 1758\n",
      "\t the 1920s\n",
      "\t the late 19th century\n",
      "\t the years\n",
      "\t the 20th century\n",
      "\t the 1970s\n",
      "\t the last ice\n",
      "\t about 52 days\n",
      "\t 4 weeks old\n",
      "CARDINAL\n",
      "\t 6.4\n",
      "\t about 30\n",
      "\t as many as 25\n",
      "\t several hundred thousand\n",
      "\t 140\n",
      "\t 11\n",
      "\t 20\n",
      "\t two\n",
      "\t dozens\n",
      "\t fewer than 200\n",
      "\t almost eradicated two\n",
      "\t one\n",
      "\t 3\n",
      "\t 50\n",
      "\t 3.5\n",
      "\t 7.1\n",
      "\t four\n",
      "\t 1,000\n",
      "\t 4-year\n",
      "\t 7.7\n",
      "\t 3.2 to 9.4\n",
      "\t 20.7\n",
      "\t 90\n",
      "\t 3.1\n",
      "\t 9.8\n",
      "\t 27\n",
      "\t 11.8\n",
      "\t 0-8018-8032-7\n",
      "\t up to 3,000\n",
      "\t 2.9\n",
      "\t 1.4 to 3.2\n",
      "\t 60\n",
      "QUANTITY\n",
      "\t 41 to\n",
      "\t 52 cm\n",
      "\t 46 to\n",
      "\t 22 in\n",
      "\t 9,800 ft\n",
      "\t 18 to\n",
      "\t 68 cm (\n",
      "\t 55 cm (\n",
      "MONEY\n",
      "\t 1,200 sq yd\n",
      "PRODUCT\n",
      "\t 3-\n",
      "ORG\n",
      "\t \n",
      "\n"
     ]
    }
   ],
   "source": [
    "arctic_fox_extract = arctic_fox['extract'].strip()\n",
    "arctic_fox_doc = nlp(arctic_fox_extract)\n",
    "\n",
    "entities = {}\n",
    "for token in arctic_fox_doc.ents:\n",
    "    entity_type_list = entities.get(token.label_, set())\n",
    "    entities[token.label_] = entity_type_list\n",
    "    entity_type_list.add(token.text)\n",
    "\n",
    "for k in entities:\n",
    "    print(k)\n",
    "    for v in entities[k]:\n",
    "        print('\\t', v)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Matchers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attributes  from `scpacy.attrs.pxd`\n",
    "```\n",
    "NULL_ATTR, IS_ALPHA, IS_ASCII, IS_DIGIT, IS_LOWER, IS_PUNCT, IS_SPACE, IS_TITLE, IS_UPPER, \n",
    "LIKE_URL, LIKE_NUM, LIKE_EMAIL, IS_STOP, IS_OOV, IS_BRACKET, IS_QUOTE, IS_LEFT_PUNCT, \n",
    "IS_RIGHT_PUNCT, ID, ORTH, LOWER, NORM, SHAPE, PREFIX, SUFFIX, LENGTH, CLUSTER, LEMMA, \n",
    "POS, TAG, DEP, ENT_IOB, ENT_TYPE, HEAD, SPACY, PROB, LANG\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(776986, 0, 0, 3)]\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add_pattern(\"HelloWorld\", [{LOWER: \"hello\"}, {IS_PUNCT: True}, {LOWER: \"world\"}])\n",
    "doc = nlp(u'Hello. world!')\n",
    "matches = matcher(doc)\n",
    "span = [(ent_id, label, start, end) for ent_id, label, start, end in matches]\n",
    "print(span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96890856024\n",
      "0.993009238879\n"
     ]
    }
   ],
   "source": [
    "ai_article = json.loads(open('./wikipedia/artificial-intelligence.json').read().lower())\n",
    "ai_extract = ai_article['extract']\n",
    "ai_doc = nlp(ai_extract)\n",
    "\n",
    "wolf_article = json.loads(open('./wikipedia/gray-wolf.json').read().lower())\n",
    "wolf_extract = wolf_article['extract']\n",
    "wolf_doc = nlp(wolf_extract)\n",
    "\n",
    "print(ai_doc.similarity(arctic_fox_doc))\n",
    "print(wolf_doc.similarity(arctic_fox_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "print(wolf_doc.vector.shape) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
