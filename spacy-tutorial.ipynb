{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaCy Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.language import EntityRecognizer\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tagger import Tagger\n",
    "from spacy.attrs import IS_PUNCT, LOWER\n",
    "from spacy.symbols import nsubj, NOUN, PUNCT, DET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"spaCy features a fast and accurate syntactic \"\\\n",
    "    \"dependency parser, and has a rich API for navigating the tree. \" \\\n",
    "    \"The parser also powers the sentence boundary detection, \"\\\n",
    "    \"and lets you iterate over base noun phrases, or 'chunks'.\"\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS (Part of Speech Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NOUN', spaCy), ('VERB', features), ('ADJ', fast), ('CCONJ', and), ('ADJ', accurate), ('ADJ', syntactic), ('NOUN', dependency), ('NOUN', parser), ('CCONJ', and), ('VERB', has), ('ADJ', rich), ('NOUN', API), ('ADP', for), ('VERB', navigating), ('NOUN', tree), ('NOUN', parser), ('ADV', also), ('VERB', powers), ('NOUN', sentence), ('ADJ', boundary), ('NOUN', detection), ('CCONJ', and), ('VERB', lets), ('PRON', you), ('VERB', iterate), ('ADP', over), ('NOUN', base), ('NOUN', noun), ('NOUN', phrases), ('CCONJ', or), ('NOUN', chunks)]\n"
     ]
    }
   ],
   "source": [
    "pairs = []\n",
    "for token in doc:\n",
    "    # Only print toknes that are not punctuation or determiners\n",
    "    if token.pos is not PUNCT and token.pos is not DET:\n",
    "        pairs.append((token.pos_, token))\n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy features a fast and accurate syntactic dependency parser, and has a rich API for navigating the tree. \n",
      "The parser also powers the sentence boundary detection, and lets you iterate over base noun phrases, or 'chunks'.\n"
     ]
    }
   ],
   "source": [
    "for token in doc.sents:\n",
    "    print(token.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to Naive Regex Sentence Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cy features a fast and accurate syntactic dependency parser, and has a rich API for navigating the tree.\n",
      "The parser also powers the sentence boundary detection, and lets you iterate over base noun phrases, or 'chunks'.\n"
     ]
    }
   ],
   "source": [
    "sentence_pattern = re.compile(r'([A-Z].*?[\\.!?])', re.M)\n",
    "sentences = sentence_pattern.findall(s)\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG API\n"
     ]
    }
   ],
   "source": [
    "for token in doc.ents:\n",
    "    print(token.label_, token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Arctic Fox wiki article and process the extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "arctic_fox = json.loads(open('./wikipedia/arctic-fox.json').read().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME\n",
      "\t 12\n",
      "ORG\n",
      "\t \n",
      "\n",
      "PRODUCT\n",
      "\t 3-\n",
      "MONEY\n",
      "\t 1,200 sq yd\n",
      "GPE\n",
      "\t canada\n",
      "\t norway\n",
      "\t russia\n",
      "\t sweden\n",
      "\t finland\n",
      "\t lapland\n",
      "\t iceland\n",
      "DATE\n",
      "\t between years due to the large population fluctuations\n",
      "\t the 1920s\n",
      "\t the late 19th century\n",
      "\t 46 to 68 cm\n",
      "\t summer\n",
      "\t 1996\n",
      "\t the last ice\n",
      "\t april and may\n",
      "\t many generations\n",
      "\t 4 weeks old\n",
      "\t several decades\n",
      "\t the end of the last ice\n",
      "\t about 52 days\n",
      "\t winter\n",
      "\t 2005\n",
      "\t five to eight kits\n",
      "\t 1997\n",
      "\t many decades\n",
      "\t each day\n",
      "\t 25 to 30\n",
      "\t the 20th century\n",
      "\t 90 years\n",
      "\t 9 weeks of age\n",
      "\t the 10th edition\n",
      "\t the last decade\n",
      "\t the 1970s\n",
      "\t 1758\n",
      "\t the years\n",
      "QUANTITY\n",
      "\t 55 cm (\n",
      "\t 52 cm\n",
      "\t 18 to\n",
      "\t 41 to\n",
      "\t 22 in\n",
      "\t 68 cm (\n",
      "\t 46 to\n",
      "\t 9,800 ft\n",
      "PERCENT\n",
      "\t more than 50%.\n",
      "\t more than\n",
      "CARDINAL\n",
      "\t 3.1\n",
      "\t 3.2 to 9.4\n",
      "\t 50\n",
      "\t as many as 25\n",
      "\t 27\n",
      "\t 3\n",
      "\t 11\n",
      "\t fewer than 200\n",
      "\t 3.5\n",
      "\t 7.7\n",
      "\t about 30\n",
      "\t 20\n",
      "\t 7.1\n",
      "\t up to 3,000\n",
      "\t two\n",
      "\t 11.8\n",
      "\t four\n",
      "\t dozens\n",
      "\t 9.8\n",
      "\t 20.7\n",
      "\t one\n",
      "\t 4-year\n",
      "\t 60\n",
      "\t 140\n",
      "\t almost eradicated two\n",
      "\t 90\n",
      "\t 2.9\n",
      "\t 0-8018-8032-7\n",
      "\t 6.4\n",
      "\t 1.4 to 3.2\n",
      "\t 1,000\n",
      "\t several hundred thousand\n"
     ]
    }
   ],
   "source": [
    "arctic_fox_extract = arctic_fox['extract'].strip()\n",
    "arctic_fox_doc = nlp(arctic_fox_extract)\n",
    "\n",
    "entities = {}\n",
    "for token in arctic_fox_doc.ents:\n",
    "    entity_type_list = entities.get(token.label_, set())\n",
    "    entities[token.label_] = entity_type_list\n",
    "    entity_type_list.add(token.text)\n",
    "\n",
    "for k in entities:\n",
    "    print(k)\n",
    "    for v in entities[k]:\n",
    "        print('\\t', v)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Matchers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attributes  from `scpacy.attrs.pxd`\n",
    "```\n",
    "NULL_ATTR, IS_ALPHA, IS_ASCII, IS_DIGIT, IS_LOWER, IS_PUNCT, IS_SPACE, IS_TITLE, IS_UPPER, \n",
    "LIKE_URL, LIKE_NUM, LIKE_EMAIL, IS_STOP, IS_OOV, IS_BRACKET, IS_QUOTE, IS_LEFT_PUNCT, \n",
    "IS_RIGHT_PUNCT, ID, ORTH, LOWER, NORM, SHAPE, PREFIX, SUFFIX, LENGTH, CLUSTER, LEMMA, \n",
    "POS, TAG, DEP, ENT_IOB, ENT_TYPE, HEAD, SPACY, PROB, LANG\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(777061, 0, 0, 3)]\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add_pattern(\"HelloWorld\", [{LOWER: \"hello\"}, {IS_PUNCT: True}, {LOWER: \"world\"}])\n",
    "doc = nlp(u'Hello. world!')\n",
    "matches = matcher(doc)\n",
    "span = [(ent_id, label, start, end) for ent_id, label, start, end in matches]\n",
    "print(span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai_doc, arctic_fox_doc: 0.96890856024\n",
      "wolf_doc, arctic_fox_doc: 0.993009238879\n"
     ]
    }
   ],
   "source": [
    "ai_article = json.loads(open('./wikipedia/artificial-intelligence.json').read().lower())\n",
    "ai_extract = ai_article['extract']\n",
    "ai_doc = nlp(ai_extract)\n",
    "\n",
    "wolf_article = json.loads(open('./wikipedia/gray-wolf.json').read().lower())\n",
    "wolf_extract = wolf_article['extract']\n",
    "wolf_doc = nlp(wolf_extract)\n",
    "\n",
    "# The similarity between two documents is a distance measure of the averages of the word vectors that make up the document\n",
    "print('ai_doc, arctic_fox_doc:', ai_doc.similarity(arctic_fox_doc))\n",
    "print('wolf_doc, arctic_fox_doc:', wolf_doc.similarity(arctic_fox_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fox, wolf 0.659473562956\n"
     ]
    }
   ],
   "source": [
    "# The similarity is a distance measure of the word vectors for each word\n",
    "fox = arctic_fox_doc[2]\n",
    "wolf = wolf_doc[5]\n",
    "print('fox, wolf', fox.similarity(wolf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artificial, humans: 0.42094545699\n",
      "artificial, animals: 0.346082360149\n",
      "animals, humans: 0.733054539968\n"
     ]
    }
   ],
   "source": [
    "artificial = ai_doc[0]\n",
    "ai = ai_doc[3]\n",
    "humans = ai_doc[19]\n",
    "animals = ai_doc[22]\n",
    "print('artificial, humans:', artificial.similarity(humans))\n",
    "print('artificial, animals:', artificial.similarity(animals))\n",
    "print('animals, humans:', animals.similarity(humans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Entities\n",
    "The following seeks to create entity recognition for units of measure. Interestingly, the results from this training can vary quite a bit from run to run. This is most likely due to the random initialization of weights in the linear models used within SpaCy. It's likely that this variation would be reduces with more training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'en'\n",
    "entity_label = 'UNIT'\n",
    "output_directory = './spacy/custom-ent-unit-model/'\n",
    "train_data = [\n",
    "    ('The bridge is 2.56 m long', [(18, 20, 'UNIT')]),\n",
    "    ('The building is 256 ft shorter than the Eifel Tower', [(20, 22, 'UNIT')]),\n",
    "    ('The desk is 3 ft wide, 2 ft deep and 3.5 ft. tall.', [(14, 16, 'UNIT'), (25, 27, 'UNIT'), (41, 44, 'UNIT')]), \n",
    "    ('The record has a diameter of 12 inches.', [(32, 37, 'UNIT')]), \n",
    "    ('It is 40 km to the next town', [(9, 11, 'UNIT')]), \n",
    "    ('It is 90 km to the next town', [(9, 11, 'UNIT')]), \n",
    "    ('I\\'ll be there in 30 minutes', [(20, 26, 'UNIT')]), \n",
    "    ('San Fransisco is about 2913.6 mi from New York City', [(29, 31, 'UNIT')]), \n",
    "    ('It\\'s 2,806 km from Austin, TX to New York City', [(11, 13, 'UNIT')]), \n",
    "    ('There are 1024 KB in an 1 MB', [[15, 17, 'UNIT'], [26, 27, 'UNIT']]),\n",
    "    ('The floppy disk can store 1024 kilobytes of data', [[31, 40, 'UNIT']]),\n",
    "    ('The hard drive on this computer can store 1 TB', [[44, 45, 'UNIT']]),\n",
    "]\n",
    "\n",
    "nlp.entity.add_label(entity_label)\n",
    "\n",
    "def train_ner(nlp, train_data, output_dir):\n",
    "    # Add new words to vocab\n",
    "    for raw_text, _ in train_data:\n",
    "        doc = nlp.make_doc(raw_text)\n",
    "        for word in doc:\n",
    "            _ = nlp.vocab[word.orth]\n",
    "\n",
    "    for itn in range(100):\n",
    "        random.shuffle(train_data)\n",
    "        for raw_text, entity_offsets in train_data:\n",
    "            doc = nlp.make_doc(raw_text)\n",
    "            gold = GoldParse(doc, entities=entity_offsets)\n",
    "            nlp.tagger(doc)\n",
    "            loss = nlp.entity.update(doc, gold)\n",
    "    nlp.end_training()\n",
    "    nlp.save_to_directory(output_dir)\n",
    "    \n",
    "ner = train_ner(nlp, train_data, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The channel is 39 km long.\n",
      "   DATE km long\n",
      "Portland, Oregon is 4400 km. from New York.\n",
      "   UNIT km\n",
      "The wall is 100 ft tall.\n",
      "   UNIT ft\n",
      "There are 2048 MB in 2 GB.\n",
      "   ORG MB\n",
      "   PRODUCT in 2\n",
      "   ORG GB\n",
      "She is 5 ft tall\n",
      "   UNIT ft\n"
     ]
    }
   ],
   "source": [
    "inputs = [\n",
    "    'The channel is 39 km long.', \n",
    "    'Portland, Oregon is 4400 km. from New York.', \n",
    "    'The wall is 100 ft tall.', \n",
    "    'There are 2048 MB in 2 GB.', \n",
    "    'She is 5 ft tall'\n",
    "]\n",
    "\n",
    "for i in inputs:\n",
    "    doc = nlp(i)\n",
    "    print(i)\n",
    "    for token in doc.ents:\n",
    "        print('  ', token.label_, token.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
